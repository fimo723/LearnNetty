# Netty学习笔记一：高并发IO的底层原理
#### 1.IO读写的基础原理

​        用户程序进行IO读写依赖于底层的IO读写，基本上会用到底层的read和write两大系统调用。在不同操作系统中，IO读写的系统调用名称可能不完全一致，但是基本功能是一样的。

​        基础知识：read系统调用，并不是直接从物理设备把数据读取到内存中；write系统调用，也不是直接把数据写入物理设备。上层应用无论是调用操作系统给的read，还是write，都会设计缓冲区。具体来说，调用操作系统的read，把数据从内核缓冲区复制到进程缓冲区；而write则是把数据从进程缓冲区复制到内核缓冲区。

​        上层程序的IO操作，实际上不是物理设备级别的读写，而是缓存的复制。read&write两大系统函数调用，都不负责数据在内核缓冲区和物理设备（如磁盘）之间的交换，这项底层的读写交换，是由操作系统内核来完成。

​        在用户程序中，无论Socket的IO、还是文件IO操作都属于上层应用的开发，他们的输入（Input）和输出（Output）的处理，在编程的流程上都是一致的。

##### 1.1内核缓冲区与进程缓冲区

​        缓冲区的目的是为了减少物理设备之间频繁地物理交换。外部设备的直接读写，涉及操作系统的中断。发生系统中断时，需保存之前的进程数据和状态信息，而结束中断之后，还需要恢复之前的进程数据和状态等信息。

内存缓冲区就是为了减少这种底层系统的时间损耗、性能损耗。

​        通过内存缓冲区，上层应用使用read系统调用时，只是把数据从内核缓冲区读取到上层应用缓冲区（进程）缓冲区；上层应用使用write系统的调用时，只是把数据从进程缓冲区复制到内核缓冲区。底层操作会对内核缓冲区进行监控，当缓冲区达到一定数量时，进行IO设备的终端处理，集中执行物理设备的IO操作，通过这种机制提升系统性能。至于什么时候进行中断（读中断、写中断），由操作系统的内核来决定，不需要用户程序关心。

​        从数量来说，在Linux系统中，操作系统只有一个内核缓冲区。而每个用户程序（进程），拥有自己的独立缓冲区，叫做进程缓冲区。因此，用户程序的IO读写程序，在大多数情况下，并没有进行实际的IO操作，而是在进程缓冲区和内核缓冲区之间进行数据交换。

##### 1.2典型的系统调用流程

​        用户程序所使用的系统调用，不等价于数据在内核缓冲区和磁盘之间的交换。read把数据从黑河缓冲区复制到进程缓冲区，write把数据从进程缓冲区复制到内核缓冲区，流程如图1所示。

![系统调用read&write流程](/Users/ygren/Library/Application Support/typora-user-images/image-20201224174329542.png)

#### 2.四种主要的IO模型

​        服务端编程，经常需要构造高性能的网络应用，需要选用高性能的IO模型。常见的IO模型有四种：

​        1.同步阻塞IO（Blocking IO）

​        阻塞与非阻塞：阻塞IO指的是需要内核IO操作彻底完成，才返回用户空间执行用户操作。阻塞指的是用户空间程序的执行状态。传统的IO模型都是同步阻塞IO。在Java中，默认创建的socket都是阻塞的。

​        同步与异步：同步IO，是一种用户空间与内核空间的IO发起方式。同步IO是指用户空间的线程是主动发起IO请求的一方，内核空间是被动接收方。异步IO则反过来，是指系统内核是主动发起IO请求的一方，用户空间的线程是被动接收方。

​        2.同步非阻塞IO（Non-blocking IO）

​        非阻塞IO，指的是用户空间的程序不需要等待内核IO操作彻底完成，可以立即返回用户空间执行用户的操作，即处于非阻塞状态，与此同时内核会立即返回用户一个状态值。

​        简单说来：阻塞是指用户空间（调用线程）一直都处于等待状态，不能干别的事情；非阻塞是指用户空间（调用线程）拿到内核返回状态值就返回自己的空间，可以去做别的事情。

​        非阻塞IO要求socket设置为NONBLOCK。，这里所说的NIO（同步非阻塞IO）模型并非Java的NIO（New IO）库。

​        3.IO多路复用（IO Multiplexing）

​        经典的Reactor反应器设计模式，有时也称为异步阻塞IO，Java中的Selector选择器和Linux中的epoll都是这种模型。

​        4.异步IO（Asynchronous IO）

​        异步IO，指的是用户空间与内核空间的调用方式反过来。用户空间的线程变成被动接受者，而内核空间成了主动调用者。有点类似于Java中比较典型的回调模式，用户空间的线程向内核空间注册了各种IO事件的回调函数，由内核区主动调用。

##### 2.1同步阻塞IO（Blocking IO）

​        在Java应用程序中，默认情况下，所有的socket连接的IO操作都是同步阻塞IO（Blocking IO）。

​        在阻塞IO模型中，Java应用程序从IO系统调用开始，直到系统调用返回，在这段时间内，Java进程是阻塞的。返回成功后，应用程序开始处理用户空间的缓存区数据。同步阻塞IO的具体流程，如图2所示。

![同步阻塞IO的流程](/Users/ygren/Library/Application Support/typora-user-images/image-20201224180001567.png)

​        阻塞IO的特点是：内核进行IO执行的两个阶段，用户线程都被占满。

​        阻塞IO的优点是：应用陈故乡开发非常简单；在阻塞等待数据期间，用户线程挂起。在阻塞期间，用户线程基本不会占用CPU资源。

​        阻塞IO的缺点是：一般情况下，会为每个连接配置一个独立的线程；反过来说，就是一个线程维护一个连接的IO操作。在并发量小的情况下，这样做没有什么问题。但是，在高并发的应用场景下，需要大量的线程来维护大量的网络连接，内存、线程却换开销会非常巨大。因此，基本上阻塞IO模型在高并发场景下是不可用的。

##### 2.2同步非阻塞IO（None Blocking IO）

​        socket连接默认是阻塞模式，在Linux系统下，可以通过设置将socket变为非阻塞模式（Non-Blocking）。使用非阻塞模式的IO读写，叫作同步非阻塞 （None Blocking IO），简称NIO模式。在NIO模式中，应用程序一旦开始IO系统调用，会出现以下两种情况：

​        （1）在内核区中没有数据的i 去那个开心系统调用会立即返回，返回一个调用失败的信息。

​        （2）在内核缓冲区中有数据的情况下，是阻塞的，知道数据从内核缓冲区复制到用户进程缓冲。复制完成后，系统调用返回成功，应用简称开始处理用户的缓存数据。

​        同步非阻塞IO的流程如图3所示。

![同步非阻塞IO的流程](/Users/ygren/Library/Application Support/typora-user-images/image-20201224180840734.png)

​        同步非阻塞IO的特点：用用程序不需要不断进行IO系统调用，轮询数据是否准备好，如果没有准备好，继续轮询，知道完成IO系统调用为止。

​        同步非阻塞IO的优点：每次发起的IO系统调用，在内核等待数据过程中可以立即返回。用户线程不会阻塞，实时性较好。

​        同步非阻塞IO的缺点：不断地轮询内核，将占用大量CPU时间，效率低下。

​        总体来说，在高并发应用场景下，同步非阻塞IO是不可用的。一般Web服务器不使用这种IO模型。这种IO模型一般很少直接使用，而是在其它IO模型中使用非阻塞IO这一特性。在Java的实际开发中，也不会涉及这种IO模型。同步非阻塞IO，可以简称为NIO，但它不是Java中的NIO，虽然英文缩写一样，但不要混淆。Java中的NIO（New IO），对用的不是四种基础IO模型中的NIO（None Blocking IO）模型，而是IO多路复用模型（IO Multiplexing）。

##### 2.3IO多路复用模型（IO Multiplexing）

​        如何避免同步非阻塞IO模型中轮询等待？IO多路复用模型解决了这一问题。

​        在IO多路复用模型中，引入了一种新的系统调用，查询IO的就绪状态。在Linux系统中，对应的系统调用为select/epoll系统调用。通过该系统调用，一个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核就能够将就绪状态返回给应用程序。随后，应用程序根据就绪的状态，进行相应的IO系统调用。

​        目前支持IO多路复用的系统调用，有select、epoll等。select系统调用，几乎在所有的操作系统上都有支持，具有良好的跨平台特性。epoll是在Linux 2.6内核中提出的，是select系统调用的Linux增强版本。

​       在IO多路复用模型中通过select/epoll系统调用，单个应用程序的线程，可以通过不断地轮询成百上千的socket连接，当某个或者某些socket网络连接有IO就绪状态，就返回对应的可执行读写操作。

​        IO多路复用模型的流程如图4所示。

![IO多路复用模型的流程](/Users/ygren/Library/Application Support/typora-user-images/image-20201224182125986.png)

​        IO多路复用模型的特点：IO多路复用模型的IO涉及两种系统调用（System Call），另一种是select/epoll（就绪查询），一种是IO操作。IO多路复用模型建立在操作系统的基础设施上，即操作系统的内核必须能够提供多路分离的系统调用select/epoll。

​        和NIO模型相似，多路复用IO也需要轮询。负责select/epoll状态查询调用的线程，需要不断地进行select/epoll轮询，查找出达到IO操作就绪的socket连接。

​        IO多路复用模型与同步非阻塞IO模型是由密切关系的。对于注册在选择器上的每一个可以查询的socket连接， 一般设置为同步非阻塞模型。这一点对用户程序而言是无感知的。

​        IO多路复用模型的优点：与一个线程维护一个连接的阻塞IO模式相比。使用select/epoll的最大优势在于，一个选择器查询线程可以同时处理成千上万个连接。系统不必创建大量的线程，也不必维护这些线程，从而大大减少了系统的开销。

​        Java语言的NIO（New IO）技术，使用的就是IO多路复用模型。在Linux系统上，使用的是epoll系统调用。

​        IO多路复用模型的缺点：本质上，select/epoll系统调用时阻塞式的，属于同步IO。都需要在读写就绪后，由系统调用本身负责进行读写，也就是说这个读写过程是阻塞的。

​        如果想彻底地解决线程的阻塞，就必须使用异步IO模型。

##### 2.4异步IO模型（Asynchronous IO）

​        异步IO模型（Asynchronous IO，简称为AIO）。AIO的基本流程是：用户线程通过系统调用，向内核注册某个IO操作。内核在整个IO操作（包括数据准备、数据复制）完成后，通知用户程序，用户执行后续的业务操作。

​        在异步IO模型中，整个内核数据处理的过程中，包括内核将数据从网络物理设备（网卡）读取到内核缓冲区、将内核缓冲区的数据复制到用户缓冲区，用户程序都不需要阻塞。

​        异步IO模型的流程如图5所示。

![异步IO模型的流程](/Users/ygren/Library/Application Support/typora-user-images/image-20201224183155248.png)

​        异步IO模型的特点：在内核等待数据和复制数据的两个阶段，用户线程都不是阻塞的。用户线程需要接收内核的IO操作完成的时间，或者用户线程需要注册一个IO操作完成的回调函数。正因为如此，异步IO有时候也被称为信号驱动IO。

​       异步IO模型的缺点：应用程序仅需要进行数据的注册与接收，其余的工作都留给了操作系统，也就是说，需要底层内核提供支持。

​        理论上来说，异步IO是真正的异步输入输出，它的吞吐量高于IO多路复用模型的吞吐量。

​        就目前而言，Windows系统下通过IOCP实现了真正的异步IO。而在Linux系统下，异步IO模型在2.6 版本中才引入，目前并不完善，其底层实现仍使用epoll，与IO多路复用相同，因此在性能上没有明显的优势。

​        大多数的高并发服务端的程序，一般都是基于Linux系统开发。因此目前这类高并发网络应用程序的开发，大多采用IO多路复用模型。

​         Netty采用高的就是IO多路复用模型，而不是异步IO模型

#### 3.通过合理配置支持百万级并发连接

​        在生产环境Linux系统中，基本上都需要解除文件句柄数的限制。原因在于，Linux系统的默认值为1024，也就是说一个进程最多接受1024个socket连接。

​        文件句柄，也叫文件描述符。在Linux系统中，文件可分为：普通文件、目录文件、链接文件和设备文件。文件描述符（FileDescriptor）是内核为了高效管理已被打开的文件所创建的索引，它是一个非负整数（通常是小整数），用于指代被打开的文件。所有的IO系统调用，包括socket的读写调用，都是通过文件描述符完成的。在Linux下，通过调用ulimit命令，可以看到单个进程能够打开的最大文件句柄数量，命令为

`ulimit -n`

ulimit命令是用来显示和修改当前用户进程一些基础限制的命令，n命令选项用于引用或设置当前的文件句柄数量的限制值。Linux的系统默认值为1024。默认的数值为1024，对绝大多数应用（例如Apache、桌面应用程序）来说已经足够了。但是，是对于一些用户基数很大的高并发应用，则是远远不够的。一个高并发的应用，面临的并发连接数往往是十万级、百万级、千万级、甚至像腾讯QQ一样的上亿级。文件句柄数不够，会导致什么后果呢？当单个进程打开的文件句柄数量，超过了系统配置的上限值时，就会发出“Socket/File:Can'topensomanyfiles”的错误提示。对于高并发、高负载的应用，就必须要调整这个系统参数，以适应处理并发处理大量连接的应用场景。可以通过ulimit来设置这两个参数。方法如下

`ulimit -n 1000000`

在上面的命令中，n的设置值越大，可以打开的文件句柄数量就越大。建议以root用户来执行此命令。然而，使用ulimit命令来修改当前用户进程的一些基础限制，仅在当前用户环境有效。直白地说，就是在当前的终端工具连接当前shell期间，修改是有效的；一旦断开连接，用户退出后，它的数值就又变回系统默认的1024了。也就是说，ulimit只能作为临时修改，系统重启后，句柄数量又会恢复为默认值。如果想永久地把设置值保存下来，可以编辑/etc/rc.local开机启动文件，在文件中添加如下内容：

`ulimit -SHn 1000000`

增加S和H两个命令选项。选项S表示软性极限值，H表示硬性极限值。硬性极限是实际的限制，就是最大可以是100万，不能再多了。软性极限是系统警告（Warning）的极限值，超过这个极限值，内核会发出警告。普通用户通过ulimit命令，可将软极限更改到硬极限的最大设置值。如果要更改硬极限，必须拥有root用户权限。终极解除Linux系统的最大文件打开数量的限制，可以通过编辑Linux的极限配置文件/etc/security/limits.conf来解决，修改此文件，加入如下内容：

`soft nofile 1000000`

`hard nofile 1000000`

softnofile表示软性极限，hardnofile表示硬性极限。在使用和安装目前非常火的分布式搜索引擎ElasticSearch，就必须去修改这个文件，增加最大的文件句柄数的极限值。在服务器运行Netty时，也需要去解除文件句柄数量的限制，修改/etc/security/limits.conf文件即可。





